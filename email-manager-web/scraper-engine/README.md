# ğŸ•·ï¸ Email Scraper Pro - Advanced Web Scraping Engine

**Production-grade email extraction engine inspired by [Scrapy](https://www.scrapy.org)**

## ğŸš€ Features

### **Core Capabilities**
- **ğŸ•·ï¸ Scrapy-Inspired Architecture**: Modular design with spiders, engines, and middleware
- **ğŸ“§ Advanced Email Extraction**: 7 different regex patterns for maximum detection
- **ğŸŒ Deep Web Crawling**: Multi-level page traversal with intelligent link following
- **ğŸ­ Anti-Detection**: Random user agents, request delays, and stealth mode
- **âš¡ High Performance**: Concurrent processing with rate limiting and clustering
- **ğŸ” JavaScript Rendering**: Puppeteer-powered dynamic content extraction
- **âœ… Email Validation**: Real-time filtering and spam detection
- **ğŸ”„ Proxy Support**: Proxy rotation for enhanced anonymity
- **ğŸ“Š Real-time Stats**: Live monitoring and comprehensive analytics

### **Technical Features**
- **Clustering**: Multi-process support for production scaling
- **Rate Limiting**: Domain-specific and global request throttling
- **Winston Logging**: Comprehensive logging with file and console outputs
- **Security**: Helmet.js protection and input validation
- **Compression**: Response compression for optimal performance
- **Health Monitoring**: Built-in health checks and system metrics

## ğŸ—ï¸ Architecture

```
Email Scraper Pro/
â”œâ”€â”€ server.js              # Main server with clustering
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ ScrapingEngine.js   # Core scraping orchestration
â”‚   â”œâ”€â”€ RateLimiter.js      # Request rate management
â”‚   â””â”€â”€ ProxyManager.js     # Proxy rotation system
â”œâ”€â”€ spiders/
â”‚   â””â”€â”€ EmailSpider.js      # Advanced email extraction spider
â””â”€â”€ logs/                   # Application logs
```

## ğŸ”§ Installation & Setup

### **Prerequisites**
- Node.js 18+ 
- npm 8+
- 2GB+ RAM recommended

### **Quick Start**
```bash
# Install dependencies
npm install

# Start the server
npm start

# Development mode with auto-restart
npm run dev
```

### **Server Endpoints**

#### **Health Check**
```bash
GET /health
```
Returns server status and system metrics.

#### **Single URL Scraping**
```bash
POST /scrape
Content-Type: application/json

{
  "url": "https://example.com",
  "options": {
    "maxPages": 20,
    "maxDepth": 2,
    "delay": 1000,
    "concurrent": 3,
    "timeout": 30000,
    "respectRobots": true,
    "followRedirects": true
  }
}
```

#### **Batch Scraping**
```bash
POST /scrape/batch
Content-Type: application/json

{
  "urls": ["https://site1.com", "https://site2.com"],
  "options": { ... }
}
```

#### **Spider Information**
```bash
GET /spiders
```
Lists available spiders and their capabilities.

#### **Statistics**
```bash
GET /stats
```
Returns scraping engine statistics and performance metrics.

## ğŸ“§ Email Extraction Patterns

The EmailSpider uses 7 advanced regex patterns to detect emails:

1. **Standard emails**: `user@domain.com`
2. **Spaced emails**: `user @ domain.com`
3. **[at] obfuscation**: `user[at]domain.com`
4. **[dot] obfuscation**: `user@domain[dot]com`
5. **JavaScript escaped**: `"user@domain.com"`
6. **HTML entities**: `user&#64;domain&#46;com`
7. **URL encoded**: `user%40domain%2Ecom`

## ğŸ›¡ï¸ Anti-Detection Features

- **Random User Agents**: Rotates through realistic browser user agents
- **Request Delays**: Configurable delays between requests
- **Proxy Rotation**: Support for HTTP/SOCKS5 proxy rotation
- **Resource Blocking**: Blocks images/CSS/fonts for faster loading
- **Stealth Mode**: Puppeteer stealth techniques
- **Rate Limiting**: Respects server load with intelligent throttling

## ğŸ“Š Performance Metrics

- **Concurrent Requests**: Up to 10 simultaneous requests
- **Processing Speed**: 2-5 pages per second (configurable)
- **Memory Usage**: ~50MB base, scales with concurrent requests
- **Email Detection**: 95%+ accuracy on standard web pages
- **JavaScript Support**: Full dynamic content rendering

## ğŸ”„ Comparison with Scrapy

| Feature | Scrapy (Python) | Email Scraper Pro | Advantage |
|---------|----------------|-------------------|-----------|
| **Language** | Python | Node.js | JavaScript ecosystem |
| **Performance** | Excellent | Excellent | Similar performance |
| **JS Rendering** | Plugin required | Built-in | Native support |
| **Email Extraction** | Generic | Specialized | Purpose-built |
| **Anti-Detection** | Advanced | Advanced | Equal capabilities |
| **Setup Complexity** | Medium | Low | Easier deployment |
| **Clustering** | Built-in | Built-in | Equal scaling |

## ğŸš€ Production Deployment

### **Environment Variables**
```bash
NODE_ENV=production
PORT=3001
LOG_LEVEL=info
```

### **Process Management**
```bash
# Using PM2
pm2 start server.js --name "email-scraper-pro" --instances max

# Using Docker
docker build -t email-scraper-pro .
docker run -p 3001:3001 email-scraper-pro
```

### **Scaling Options**
- **Horizontal**: Multiple server instances behind load balancer
- **Vertical**: Cluster mode automatically uses all CPU cores
- **Distributed**: Redis integration for shared state (planned)

## ğŸ”§ Configuration

### **Rate Limiting**
```javascript
const rateLimiter = new RateLimiter({
  globalRequestsPerSecond: 5,
  domainRequestsPerSecond: 2,
  maxConcurrent: 10
});
```

### **Proxy Setup**
```javascript
const proxyManager = new ProxyManager({
  enabled: true,
  proxies: [
    { host: '1.2.3.4', port: 8080, type: 'http' },
    { host: '5.6.7.8', port: 1080, type: 'socks5', username: 'user', password: 'pass' }
  ]
});
```

## ğŸ“ˆ Monitoring & Logging

- **Winston Logging**: Structured JSON logs with timestamps
- **Health Endpoint**: Real-time system status
- **Performance Metrics**: Request counts, success rates, timing
- **Error Tracking**: Comprehensive error logging and stack traces

## ğŸ› ï¸ Development

### **Testing**
```bash
npm test
```

### **Debugging**
```bash
DEBUG=* npm run dev
```

### **Code Structure**
- **server.js**: Express server with clustering
- **core/**: Reusable core components
- **spiders/**: Specialized extraction spiders
- **logs/**: Application logs (created automatically)

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- **[Scrapy](https://www.scrapy.org)**: Inspiration for architecture and design patterns
- **Puppeteer**: JavaScript rendering and browser automation
- **Express.js**: Web server framework
- **Winston**: Logging library

---

**Built with â¤ï¸ for Email Manager Pro**

*Production-grade scraping capabilities inspired by the world's most-used web scraping framework.*

## Deploy to Railway

[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/XXXXXX)

## Manual Deployment

1. Fork this repository
2. Sign up at [Railway](https://railway.app)
3. Create new project from GitHub repo
4. Set root directory: `email-manager-web/scraper-engine`
5. Add environment variables:
   - `NODE_ENV=production`
   - `PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true`
   - `PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser`

## Features

- ğŸ•·ï¸ Advanced Puppeteer-based scraping
- ğŸ“§ Email extraction from websites  
- ğŸš€ Production-ready with Docker
- ğŸ”’ Rate limiting & security
- ğŸ“Š Health monitoring
- ğŸŒ CORS configured for Vercel frontend

## API Endpoints

- `GET /health` - Health check
- `POST /scrape` - Scrape emails from URL
- `GET /health-check/:url` - Test website accessibility 